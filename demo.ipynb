{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving main subject problem\n",
    "\n",
    "This could be a way to solve annotation problem, or it could be a complete overkill!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Annotate the data with the main subject in the image (maybe this is overkill?)\"\"\"\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "env_file = os.path.join(os.path.dirname(__file__), \"resources/config.env\")\n",
    "load_dotenv(env_file)\n",
    "\n",
    "import cv2\n",
    "import photomatcher.model.yunet as yunet\n",
    "import photomatcher.utils as utils\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "import copy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load pre-trained MobileNetV3 Small model\n",
    "mobilenet_v3_small = models.mobilenet_v3_small(pretrained=True)\n",
    "mobilenet_v3_small = mobilenet_v3_small.eval() \n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "def expand_bounding_box(face, expansion_factor=0.4):\n",
    "    \"\"\"Expand the bounding box of the face in all directions.\"\"\"\n",
    "    x, y, w, h = face[:4]\n",
    "    expand_w = int(w * expansion_factor)\n",
    "    expand_h = int(h * expansion_factor)\n",
    "    x_new = max(0, x - expand_w)\n",
    "    y_new = max(0, y - expand_h)\n",
    "    w_new = w + 2 * expand_w\n",
    "    h_new = h + 2 * expand_h\n",
    "    return (int(x_new), int(y_new), int(w_new), int(h_new))\n",
    "\n",
    "def extract_features(region):\n",
    "    \"\"\"Extract features from an image region using MobileNetV3 Small.\"\"\"\n",
    "    image = Image.fromarray(region).convert('RGB')\n",
    "    image = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = mobilenet_v3_small(image).numpy().flatten()\n",
    "    return features\n",
    "\n",
    "def display_image_with_bbox(image, bbox):\n",
    "    \"\"\"Display an image with bounding box using PIL.\"\"\"\n",
    "    x, y, w, h = bbox\n",
    "    img_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    draw.rectangle([x, y, x + w, y + h], outline=\"red\", width=2)\n",
    "    img_pil.show()\n",
    "\n",
    "def create_dataset(image_path: str, output_path: str):\n",
    "    \"\"\"Create dataset from the given path iteratively.\"\"\"\n",
    "    \n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    images = utils.search_all_images(image_path)\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "    for image_path in images:\n",
    "        cache_file = os.path.join(output_path, f'{os.path.basename(image_path)}.npz')\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            print(f\"Skipping {image_path}, already processed.\")\n",
    "            continue\n",
    "\n",
    "        face_table = yunet.run_face_detection(image_path)\n",
    "        faces = face_table['faces']\n",
    "        original_image = face_table['image']\n",
    "        image_height, image_width, _ = original_image.shape\n",
    "       \n",
    "        faces = sorted(faces, key=lambda face: face[2] * face[3], reverse=True)[:3]\n",
    "\n",
    "        print(f\"Processing {image_path} with {len(faces)} faces.\")\n",
    "\n",
    "        for face in faces:\n",
    "            x, y, w, h = face[:4]\n",
    "\n",
    "            image = copy.deepcopy(original_image)\n",
    "            expanded_face = expand_bounding_box(face)\n",
    "            x_exp, y_exp, w_exp, h_exp = expanded_face\n",
    "\n",
    "            # Ensure expanded face region is within image boundaries\n",
    "            x_exp = max(0, min(x_exp, image_width - w_exp))\n",
    "            y_exp = max(0, min(y_exp, image_height - h_exp))\n",
    "\n",
    "            face_region = image[y_exp:y_exp+h_exp, x_exp:x_exp+w_exp]\n",
    "\n",
    "            # Visualize the expanded face region\n",
    "            display_image_with_bbox(image, (x_exp, y_exp, w_exp, h_exp))\n",
    "\n",
    "            # Ask for user input to label the face\n",
    "            label = input(f\"Is this the main subject in {image_path}? (1 for yes, 0 for no): \")\n",
    "            label = int(label)\n",
    "\n",
    "            # Extract features and create feature vector\n",
    "            features = extract_features(face_region)\n",
    "            feature_vector = np.concatenate(([image_width, image_height, x, y, w, h], features))\n",
    "            print(f\"Image width: {image_width}, Image height: {image_height}\")\n",
    "            print(f\"Face bounding box: {x}, {y}, {w}, {h}\")\n",
    "            print(f\"Extracted features: {features.shape}\")\n",
    "            print(f\"Concatenated feature vector: {feature_vector.shape}\")\n",
    "            print('-' * 50)\n",
    "\n",
    "\n",
    "            X.append(feature_vector)\n",
    "            Y.append(label)\n",
    "\n",
    "        # Save progress to cache file\n",
    "        np.savez(cache_file, X=np.array(X), y=np.array(Y))\n",
    "        print(f\"Saved {cache_file}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_path_a = '/home/chophilip21/togamatcher/dataset/Secondary/On Stage'\n",
    "    output_path = '/home/chophilip21/togamatcher/dataset/training_data'\n",
    "\n",
    "    create_dataset(dataset_path_a, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "beeware-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
